{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Model - CIFAR-10 Dataset\n",
    "## Minor Project: Model Improvement Exercise\n",
    "\n",
    "This notebook contains a baseline image classification model using the CIFAR-10 dataset. Your goal is to improve the model's performance through various techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset\n",
    "\n",
    "CIFAR-10 consists of 60,000 32x32 color images in 10 classes:\n",
    "- Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Class names\n",
    "class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', \n",
    "               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"\\nNumber of classes: {len(class_names)}\")\n",
    "print(f\"Pixel value range: [{X_train.min()}, {X_train.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.title(class_names[y_train[i][0]], fontsize=10)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar([class_names[i] for i in unique], counts, color='steelblue')\n",
    "plt.xlabel('Classes', fontsize=12)\n",
    "plt.ylabel('Number of Images', fontsize=12)\n",
    "plt.title('Class Distribution in Training Set', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_categorical = to_categorical(y_train, num_classes=10)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"Normalized training data shape: {X_train_normalized.shape}\")\n",
    "print(f\"One-hot encoded labels shape: {y_train_categorical.shape}\")\n",
    "print(f\"Sample label (original): {y_train[0]}\")\n",
    "print(f\"Sample label (one-hot): {y_train_categorical[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model (Simple CNN)\n",
    "\n",
    "**Current Architecture:**\n",
    "- 2 Convolutional layers with max pooling\n",
    "- Flatten layer\n",
    "- 1 Dense layer\n",
    "- Output layer\n",
    "\n",
    "**Areas for Improvement:**\n",
    "1. Add more convolutional layers\n",
    "2. Add batch normalization\n",
    "3. Add dropout for regularization\n",
    "4. Experiment with different optimizers\n",
    "5. Use data augmentation\n",
    "6. Try transfer learning (VGG, ResNet, etc.)\n",
    "7. Adjust learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_model():\n",
    "    \"\"\"\n",
    "    Creates a simple baseline CNN model.\n",
    "    This is intentionally basic to give room for improvement.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the baseline model\n",
    "baseline_model = create_baseline_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compile and Train the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_baseline_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = baseline_model.fit(\n",
    "    X_train_normalized,\n",
    "    y_train_categorical,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Baseline Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = baseline_model.evaluate(X_test_normalized, y_test_categorical, verbose=0)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BASELINE MODEL PERFORMANCE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = baseline_model.predict(X_test_normalized)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test.flatten()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Baseline Model', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def plot_predictions(images, true_labels, pred_labels, class_names, num_samples=12):\n",
    "    \"\"\"\n",
    "    Plot sample images with their true and predicted labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        \n",
    "        true_label = class_names[true_labels[i]]\n",
    "        pred_label = class_names[pred_labels[i]]\n",
    "        \n",
    "        color = 'green' if true_labels[i] == pred_labels[i] else 'red'\n",
    "        plt.title(f\"True: {true_label}\\nPred: {pred_label}\", \n",
    "                 fontsize=10, color=color, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot some predictions\n",
    "plot_predictions(X_test[:12], y_true_classes[:12], y_pred_classes[:12], class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improved Model Section\n",
    "\n",
    "**TODO: Your task is to improve upon the baseline model.**\n",
    "\n",
    "### Suggested Improvements:\n",
    "\n",
    "1. **Architecture Improvements:**\n",
    "   - Add more convolutional layers (3-5 blocks)\n",
    "   - Increase number of filters progressively (32 → 64 → 128 → 256)\n",
    "   - Add batch normalization after each conv layer\n",
    "   - Add dropout layers (0.2-0.5) to prevent overfitting\n",
    "   - Try residual connections (ResNet-style)\n",
    "\n",
    "2. **Data Augmentation:**\n",
    "   ```python\n",
    "   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "   \n",
    "   datagen = ImageDataGenerator(\n",
    "       rotation_range=15,\n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       zoom_range=0.1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Transfer Learning:**\n",
    "   - Use pre-trained models: VGG16, ResNet50, EfficientNet\n",
    "   - Fine-tune the last few layers\n",
    "\n",
    "4. **Hyperparameter Tuning:**\n",
    "   - Learning rate: Try 0.001, 0.0001\n",
    "   - Batch size: Try 32, 128, 256\n",
    "   - Optimizers: Adam, SGD with momentum, RMSprop\n",
    "\n",
    "5. **Advanced Techniques:**\n",
    "   - Learning rate scheduling (cosine annealing)\n",
    "   - Mixed precision training\n",
    "   - Ensemble methods\n",
    "\n",
    "### Implementation Space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your improved model here\n",
    "\n",
    "def create_improved_model():\n",
    "    \"\"\"\n",
    "    Create an improved version of the baseline model.\n",
    "    \n",
    "    Implement your improvements here!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: A more sophisticated architecture\n",
    "    model = models.Sequential([\n",
    "        # First block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Second block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Third block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Uncomment to create and view the improved model\n",
    "# improved_model = create_improved_model()\n",
    "# improved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement data augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Visualize augmented images\n",
    "# sample_image = X_train_normalized[0:1]\n",
    "# plt.figure(figsize=(15, 3))\n",
    "# plt.subplot(1, 6, 1)\n",
    "# plt.imshow(sample_image[0])\n",
    "# plt.title('Original')\n",
    "# plt.axis('off')\n",
    "\n",
    "# i = 2\n",
    "# for batch in datagen.flow(sample_image, batch_size=1):\n",
    "#     plt.subplot(1, 6, i)\n",
    "#     plt.imshow(batch[0])\n",
    "#     plt.title(f'Augmented {i-1}')\n",
    "#     plt.axis('off')\n",
    "#     i += 1\n",
    "#     if i > 6:\n",
    "#         break\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile and train your improved model\n",
    "\n",
    "# Example compilation\n",
    "# improved_model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# Example training with data augmentation\n",
    "# history_improved = improved_model.fit(\n",
    "#     datagen.flow(X_train_normalized, y_train_categorical, batch_size=64),\n",
    "#     epochs=50,\n",
    "#     validation_data=(X_test_normalized, y_test_categorical),\n",
    "#     callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate and compare your improved model with the baseline\n",
    "\n",
    "# Example evaluation\n",
    "# test_loss_improved, test_accuracy_improved = improved_model.evaluate(\n",
    "#     X_test_normalized, y_test_categorical, verbose=0\n",
    "# )\n",
    "\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(f\"MODEL COMPARISON\")\n",
    "# print(f\"{'='*60}\")\n",
    "# print(f\"Baseline Model - Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "# print(f\"Improved Model - Test Accuracy: {test_accuracy_improved:.4f} ({test_accuracy_improved*100:.2f}%)\")\n",
    "# print(f\"Improvement: {(test_accuracy_improved - test_accuracy)*100:.2f}%\")\n",
    "# print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transfer Learning (Advanced)\n",
    "\n",
    "For even better performance, try using pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement transfer learning (Optional)\n",
    "\n",
    "# Example using EfficientNetB0\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "# from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# def create_transfer_learning_model():\n",
    "#     # Load pre-trained model\n",
    "#     base_model = EfficientNetB0(\n",
    "#         include_top=False,\n",
    "#         weights='imagenet',\n",
    "#         input_shape=(32, 32, 3)\n",
    "#     )\n",
    "#     \n",
    "#     # Freeze base model layers\n",
    "#     base_model.trainable = False\n",
    "#     \n",
    "#     # Add custom classification head\n",
    "#     inputs = keras.Input(shape=(32, 32, 3))\n",
    "#     x = base_model(inputs, training=False)\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     outputs = Dense(10, activation='softmax')(x)\n",
    "#     \n",
    "#     model = keras.Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "# transfer_model = create_transfer_learning_model()\n",
    "# transfer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Your Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "# baseline_model.save('final_baseline_model.h5')\n",
    "# improved_model.save('final_improved_model.h5')\n",
    "\n",
    "# Or save in TensorFlow SavedModel format\n",
    "# baseline_model.save('saved_models/baseline_model')\n",
    "# improved_model.save('saved_models/improved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Next Steps\n",
    "\n",
    "### Summary of Improvements:\n",
    "- Document what improvements you tried\n",
    "- Note which techniques worked best\n",
    "- Record your final accuracy improvements\n",
    "\n",
    "### Future Work:\n",
    "1. Try ensemble methods (combine multiple models)\n",
    "2. Experiment with different architectures (DenseNet, ResNet, Vision Transformers)\n",
    "3. Use techniques like mixup or cutmix\n",
    "4. Apply neural architecture search (NAS)\n",
    "5. Test on other datasets (CIFAR-100, ImageNet subset)\n",
    "\n",
    "### Resources:\n",
    "- [TensorFlow Documentation](https://www.tensorflow.org/)\n",
    "- [Keras Applications](https://keras.io/api/applications/)\n",
    "- [Papers with Code](https://paperswithcode.com/) - Latest SOTA models\n",
    "- [Fast.ai](https://www.fast.ai/) - Practical deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
