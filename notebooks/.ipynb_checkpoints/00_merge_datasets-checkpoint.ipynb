{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š Multi-Dataset Merger - Student Dropout Prediction\n",
                "## Combining 5 Datasets for Maximum Accuracy\n",
                "\n",
                "This notebook merges multiple student dropout datasets:\n",
                "1. **Dataset 1**: Higher Education Predictors (4,400 students)\n",
                "2. **Dataset 2**: Student Performance - Math (395 students)\n",
                "3. **Dataset 3**: Academic Success - Dropout Prediction (4,424 students)\n",
                "4. **Dataset 4**: Student Mental Health (101 students)\n",
                "5. **Dataset 5**: Predict Students Dropout & Success (4,424 students)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add parent directory to path\n",
                "sys.path.append('..')\n",
                "import config\n",
                "\n",
                "print(\"âœ“ Libraries imported successfully\")\n",
                "print(f\"Working Directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load All Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load each dataset\n",
                "datasets = {}\n",
                "\n",
                "for idx, (name, path) in enumerate(zip(['dataset1', 'dataset2', 'dataset3', 'dataset4', 'dataset5'], \n",
                "                                        config.ALL_DATASETS), 1):\n",
                "    try:\n",
                "        df = pd.read_csv(path)\n",
                "        datasets[name] = df\n",
                "        print(f\"âœ“ {config.DATASET_NAMES[name]:40} - {df.shape[0]:5,} rows Ã— {df.shape[1]:2} columns\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"âœ— {config.DATASET_NAMES[name]:40} - FILE NOT FOUND\")\n",
                "    except Exception as e:\n",
                "        print(f\"âœ— {config.DATASET_NAMES[name]:40} - ERROR: {str(e)}\")\n",
                "\n",
                "print(f\"\\nðŸ“Š Total datasets loaded: {len(datasets)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Explore Dataset Structures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display columns for each dataset\n",
                "for name, df in datasets.items():\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"{config.DATASET_NAMES[name]:^80}\")\n",
                "    print(f\"{'='*80}\")\n",
                "    print(f\"Shape: {df.shape}\")\n",
                "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
                "    print(df.columns.tolist())\n",
                "    print(f\"\\nFirst few rows:\")\n",
                "    display(df.head(2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Identify Target Variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check target variables in each dataset\n",
                "print(\"Target Variable Analysis:\\n\")\n",
                "\n",
                "for name, df in datasets.items():\n",
                "    print(f\"\\n{config.DATASET_NAMES[name]}:\")\n",
                "    \n",
                "    # Check for common target column names\n",
                "    target_candidates = ['Status', 'Target', 'Dropout', 'target', 'status', 'dropout']\n",
                "    \n",
                "    found = False\n",
                "    for col in df.columns:\n",
                "        if any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
                "            print(f\"  Potential Target Column: '{col}'\")\n",
                "            print(f\"  Unique Values: {df[col].unique()}\")\n",
                "            print(f\"  Value Counts:\\n{df[col].value_counts()}\")\n",
                "            found = True\n",
                "            break\n",
                "    \n",
                "    if not found:\n",
                "        print(f\"  âš ï¸ No obvious target column found\")\n",
                "        print(f\"  Columns available: {df.columns.tolist()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Standardize Target Variables\n",
                "\n",
                "Create a unified 'Target' column with standard values: Dropout, Graduate, Enrolled"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to standardize target variable\n",
                "def standardize_target(df, target_col=None):\n",
                "    \"\"\"\n",
                "    Standardize target variable to: Dropout, Graduate, Enrolled\n",
                "    \"\"\"\n",
                "    df_copy = df.copy()\n",
                "    \n",
                "    if target_col is None:\n",
                "        # Try to find target column automatically\n",
                "        target_candidates = ['Status', 'Target', 'Dropout', 'target', 'status', 'dropout']\n",
                "        for col in df_copy.columns:\n",
                "            if any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
                "                target_col = col\n",
                "                break\n",
                "    \n",
                "    if target_col is None:\n",
                "        print(f\"âš ï¸ No target column found. Skipping...\")\n",
                "        return df_copy\n",
                "    \n",
                "    # Rename to 'Target'\n",
                "    df_copy = df_copy.rename(columns={target_col: 'Target'})\n",
                "    \n",
                "    # Standardize values\n",
                "    df_copy['Target'] = df_copy['Target'].astype(str).str.strip()\n",
                "    \n",
                "    return df_copy\n",
                "\n",
                "# Apply standardization\n",
                "standardized_datasets = {}\n",
                "for name, df in datasets.items():\n",
                "    standardized_datasets[name] = standardize_target(df)\n",
                "    if 'Target' in standardized_datasets[name].columns:\n",
                "        print(f\"âœ“ {config.DATASET_NAMES[name]:40} - Target standardized\")\n",
                "        print(f\"  Values: {standardized_datasets[name]['Target'].unique()}\")\n",
                "    else:\n",
                "        print(f\"âœ— {config.DATASET_NAMES[name]:40} - No target found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Identify Common Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find common features across datasets\n",
                "from collections import Counter\n",
                "\n",
                "all_columns = []\n",
                "for df in standardized_datasets.values():\n",
                "    all_columns.extend([col.lower().replace(' ', '_') for col in df.columns])\n",
                "\n",
                "column_frequency = Counter(all_columns)\n",
                "\n",
                "print(\"\\nColumn Frequency Across Datasets:\\n\")\n",
                "for col, count in column_frequency.most_common(20):\n",
                "    print(f\"{col:40} - appears in {count}/{len(datasets)} datasets\")\n",
                "\n",
                "# Identify core features present in at least 2 datasets\n",
                "core_features = [col for col, count in column_frequency.items() if count >= 2]\n",
                "print(f\"\\nâœ“ Found {len(core_features)} core features appearing in 2+ datasets\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Harmonization\n",
                "\n",
                "Create a common feature set across all datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define key features to extract/engineer\n",
                "KEY_FEATURES = [\n",
                "    'age',\n",
                "    'gender',\n",
                "    'course',\n",
                "    'year_of_study',\n",
                "    'previous_qualification',\n",
                "    'admission_grade',\n",
                "    'mother_qualification',\n",
                "    'father_qualification',\n",
                "    'tuition_fees_up_to_date',\n",
                "    'scholarship_holder',\n",
                "    'debtor',\n",
                "    'curricular_units_1st_sem',\n",
                "    'curricular_units_2nd_sem',\n",
                "    'unemployment_rate',\n",
                "    'inflation_rate',\n",
                "    'gdp'\n",
                "]\n",
                "\n",
                "def harmonize_features(df, dataset_name):\n",
                "    \"\"\"\n",
                "    Extract and harmonize features from dataset\n",
                "    \"\"\"\n",
                "    harmonized = pd.DataFrame()\n",
                "    \n",
                "    # Add dataset source\n",
                "    harmonized['dataset_source'] = dataset_name\n",
                "    \n",
                "    # Map columns to standard names\n",
                "    column_mapping = {}\n",
                "    \n",
                "    for col in df.columns:\n",
                "        col_lower = col.lower().replace(' ', '_')\n",
                "        \n",
                "        # Age\n",
                "        if 'age' in col_lower:\n",
                "            harmonized['age'] = df[col]\n",
                "        \n",
                "        # Gender\n",
                "        elif 'gender' in col_lower or 'sex' in col_lower:\n",
                "            harmonized['gender'] = df[col]\n",
                "        \n",
                "        # Course\n",
                "        elif 'course' in col_lower:\n",
                "            harmonized['course'] = df[col]\n",
                "        \n",
                "        # Add more mappings as needed\n",
                "    \n",
                "    # Add target if available\n",
                "    if 'Target' in df.columns:\n",
                "        harmonized['Target'] = df['Target']\n",
                "    \n",
                "    # Fill missing columns with NaN\n",
                "    for feature in KEY_FEATURES:\n",
                "        if feature not in harmonized.columns:\n",
                "            harmonized[feature] = np.nan\n",
                "    \n",
                "    return harmonized\n",
                "\n",
                "print(\"Harmonizing datasets...\\n\")\n",
                "harmonized_datasets = {}\n",
                "for name, df in standardized_datasets.items():\n",
                "    harmonized_datasets[name] = harmonize_features(df, config.DATASET_NAMES[name])\n",
                "    print(f\"âœ“ {config.DATASET_NAMES[name]:40} - {harmonized_datasets[name].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Merge Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Concatenate all harmonized datasets\n",
                "merged_df = pd.concat(harmonized_datasets.values(), ignore_index=True)\n",
                "\n",
                "print(f\"\"\"\\n{'='*80}\n",
                "MERGED DATASET SUMMARY\n",
                "{'='*80}\n",
                "Total Rows: {merged_df.shape[0]:,}\n",
                "Total Columns: {merged_df.shape[1]}\n",
                "{'='*80}\n",
                "\"\"\")\n",
                "\n",
                "# Show dataset distribution\n",
                "print(\"\\nDataset Distribution:\")\n",
                "print(merged_df['dataset_source'].value_counts())\n",
                "\n",
                "# Show target distribution\n",
                "if 'Target' in merged_df.columns:\n",
                "    print(\"\\nTarget Distribution:\")\n",
                "    print(merged_df['Target'].value_counts())\n",
                "\n",
                "# Show missing values\n",
                "print(\"\\nMissing Values:\")\n",
                "missing = merged_df.isnull().sum()\n",
                "missing = missing[missing > 0].sort_values(ascending=False)\n",
                "print(missing.head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Basic Data Cleaning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove columns with >70% missing values\n",
                "threshold = 0.7\n",
                "missing_pct = merged_df.isnull().sum() / len(merged_df)\n",
                "cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
                "\n",
                "print(f\"Dropping {len(cols_to_drop)} columns with >{threshold*100}% missing values:\")\n",
                "print(cols_to_drop)\n",
                "\n",
                "merged_df = merged_df.drop(columns=cols_to_drop)\n",
                "\n",
                "# Remove rows without target variable\n",
                "if 'Target' in merged_df.columns:\n",
                "    before = len(merged_df)\n",
                "    merged_df = merged_df.dropna(subset=['Target'])\n",
                "    print(f\"\\nRemoved {before - len(merged_df):,} rows without target variable\")\n",
                "\n",
                "print(f\"\\nFinal merged dataset shape: {merged_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Merged Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save merged dataset\n",
                "output_path = config.MERGED_DATASET_PATH\n",
                "merged_df.to_csv(output_path, index=False)\n",
                "\n",
                "print(f\"\\nâœ“ Merged dataset savedto: {output_path}\")\n",
                "print(f\"  Total records: {len(merged_df):,}\")\n",
                "print(f\"  Total features: {merged_df.shape[1]}\")\n",
                "print(f\"  File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
                "\n",
                "# Display sample\n",
                "print(\"\\nSample of merged data:\")\n",
                "display(merged_df.head())\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"âœ“ DATASET MERGING COMPLETE!\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Run 02_data_preprocessing.ipynb for detailed preprocessing\")\n",
                "print(\"2. Train models with the merged dataset\")\n",
                "print(\"3. Expect improved accuracy due to larger sample size\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}