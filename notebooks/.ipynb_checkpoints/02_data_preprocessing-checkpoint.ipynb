{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§¹ Data Preprocessing & Feature Engineering\n",
                "## Student Dropout Prediction Project\n",
                "\n",
                "**Goal:** Prepare the merged dataset for machine learning by handling missing values, encoding features, scaling, and balancing classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import sys\n",
                "import os\n",
                "import importlib\n",
                "import warnings\n",
                "import joblib\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add parent directory to path\n",
                "sys.path.append('..')\n",
                "import config\n",
                "importlib.reload(config)\n",
                "\n",
                "print(\"âœ“ Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Merged Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    df = pd.read_csv(config.MERGED_DATASET_PATH)\n",
                "    print(f\"âœ“ Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
                "except FileNotFoundError:\n",
                "    print(\"âŒ Error: Merged dataset not found. Run 00_merge_datasets_v2.ipynb first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Handle Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X = df.drop(columns=['Target', 'Dataset_Source'])\n",
                "y = df['Target']\n",
                "\n",
                "# Identify numerical and categorical columns\n",
                "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
                "categorical_cols = X.select_dtypes(include=['object']).columns\n",
                "\n",
                "print(f\"Numerical Features: {len(numerical_cols)}\")\n",
                "print(f\"Categorical Features: {len(categorical_cols)}\")\n",
                "\n",
                "# Define Imputers\n",
                "# Numerical -> Median (robust to outliers)\n",
                "num_imputer = SimpleImputer(strategy='median')\n",
                "\n",
                "# Categorical -> Most Frequent (Mode)\n",
                "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
                "\n",
                "# Apply Imputation\n",
                "X[numerical_cols] = num_imputer.fit_transform(X[numerical_cols])\n",
                "X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
                "\n",
                "print(\"âœ“ Missing values imputed\")\n",
                "print(f\"Remaining missing: {X.isnull().sum().sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Encoding & Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode Target Variable\n",
                "target_encoder = LabelEncoder()\n",
                "y_encoded = target_encoder.fit_transform(y)\n",
                "print(f\"Target Classes: {target_encoder.classes_}\")\n",
                "\n",
                "# Preprocessing Pipeline\n",
                "# Numerical -> Scale\n",
                "# Categorical -> One-Hot Encode\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), numerical_cols),\n",
                "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Fit and Transform\n",
                "X_processed = preprocessor.fit_transform(X)\n",
                "\n",
                "# Get feature names after encoding\n",
                "feature_names = list(numerical_cols) + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
                "\n",
                "print(f\"âœ“ Data processed. New shape: {X_processed.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Handle Class Imbalance (SMOTE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Original Class Distribution:\")\n",
                "print(pd.Series(y_encoded).value_counts())\n",
                "\n",
                "# Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
                "smote = SMOTE(random_state=config.RANDOM_STATE)\n",
                "X_resampled, y_resampled = smote.fit_resample(X_processed, y_encoded)\n",
                "\n",
                "print(\"\\nResampled Class Distribution:\")\n",
                "print(pd.Series(y_resampled).value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train-Test Split & Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_resampled, y_resampled, \n",
                "    test_size=config.TEST_SIZE, \n",
                "    random_state=config.RANDOM_STATE, \n",
                "    stratify=y_resampled\n",
                ")\n",
                "\n",
                "print(f\"Train Shape: {X_train.shape}\")\n",
                "print(f\"Test Shape:  {X_test.shape}\")\n",
                "\n",
                "# Save processed data\n",
                "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
                "train_df['Target'] = y_train\n",
                "\n",
                "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
                "test_df['Target'] = y_test\n",
                "\n",
                "train_df.to_csv(config.TRAIN_DATA_PATH, index=False)\n",
                "test_df.to_csv(config.TEST_DATA_PATH, index=False)\n",
                "\n",
                "# Save preprocessor objects for later use (inference)\n",
                "joblib.dump(preprocessor, config.MODEL_DIR / 'preprocessor.pkl')\n",
                "joblib.dump(target_encoder, config.MODEL_DIR / 'target_encoder.pkl')\n",
                "\n",
                "print(f\"\\nâœ“ Data saved to:\\n  - {config.TRAIN_DATA_PATH}\\n  - {config.TEST_DATA_PATH}\")\n",
                "print(f\"âœ“ Preprocessors saved to {config.MODEL_DIR}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}