{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü§ñ Model Training & Evaluation\n",
                "## Student Dropout Prediction Project\n",
                "\n",
                "**Goal:** Train multiple machine learning models, compare their performance, and select the best one for deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from xgboost import XGBClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
                "from sklearn.model_selection import cross_val_score\n",
                "import joblib\n",
                "import sys\n",
                "import os\n",
                "import importlib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add parent directory to path\n",
                "sys.path.append('..')\n",
                "import config\n",
                "importlib.reload(config)\n",
                "\n",
                "print(\"‚úì Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    train_df = pd.read_csv(config.TRAIN_DATA_PATH)\n",
                "    test_df = pd.read_csv(config.TEST_DATA_PATH)\n",
                "    \n",
                "    X_train = train_df.drop(columns=['Target'])\n",
                "    y_train = train_df['Target']\n",
                "    X_test = test_df.drop(columns=['Target'])\n",
                "    y_test = test_df['Target']\n",
                "    \n",
                "    print(f\"‚úì Data loaded successfully\")\n",
                "    print(f\"Train shape: {X_train.shape}\")\n",
                "    print(f\"Test shape:  {X_test.shape}\")\n",
                "except FileNotFoundError:\n",
                "    print(\"‚ùå Error: Processed data not found. Run 02_data_preprocessing.ipynb first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(**config.LR_PARAMS),\n",
                "    'Decision Tree': DecisionTreeClassifier(**config.DT_PARAMS),\n",
                "    'Random Forest': RandomForestClassifier(**config.RF_PARAMS),\n",
                "    'XGBoost': XGBClassifier(**config.XGBOOST_PARAMS),\n",
                "    'SVM': SVC(**config.SVM_PARAMS, probability=True)\n",
                "}\n",
                "\n",
                "print(f\"Defined {len(models)} models for training.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train and Evaluate Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "trained_models = {}\n",
                "\n",
                "print(\"Training models...\\n\")\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"‚è≥ Training {name}...\")\n",
                "    \n",
                "    # Train\n",
                "    model.fit(X_train, y_train)\n",
                "    trained_models[name] = model\n",
                "    \n",
                "    # Predict\n",
                "    y_pred = model.predict(X_test)\n",
                "    \n",
                "    # Metrics\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
                "    \n",
                "    # Cross-Validation (5-fold)\n",
                "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
                "    cv_acc = cv_scores.mean()\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Test Accuracy': acc,\n",
                "        'CV Accuracy': cv_acc,\n",
                "        'F1 Score': f1\n",
                "    })\n",
                "    \n",
                "    print(f\"   ‚úì Test Acc: {acc:.4f} | CV Acc: {cv_acc:.4f}\")\n",
                "\n",
                "# Create comparison dataframe\n",
                "results_df = pd.DataFrame(results).sort_values(by='Test Accuracy', ascending=False)\n",
                "print(\"\\nüèÜ Model Comparison:\")\n",
                "display(results_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualize Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x='Test Accuracy', y='Model', data=results_df, palette='viridis')\n",
                "plt.title('Model Accuracy Comparison')\n",
                "plt.xlim(0, 1.0)\n",
                "plt.axvline(x=0.83, color='r', linestyle='--', label='Target Accuracy (83%)')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Confusion Matrix (Best Model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_model_name = results_df.iloc[0]['Model']\n",
                "best_model = trained_models[best_model_name]\n",
                "\n",
                "print(f\"Detailed Report for Best Model: {best_model_name}\")\n",
                "y_pred_best = best_model.predict(X_test)\n",
                "print(classification_report(y_test, y_pred_best, target_names=['Dropout', 'Enrolled', 'Graduate']))\n",
                "\n",
                "# Plot Confusion Matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "cm = confusion_matrix(y_test, y_pred_best)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['Dropout', 'Enrolled', 'Graduate'],\n",
                "            yticklabels=['Dropout', 'Enrolled', 'Graduate'])\n",
                "plt.title(f'Confusion Matrix - {best_model_name}')\n",
                "plt.ylabel('True Label')\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if hasattr(best_model, 'feature_importances_'):\n",
                "    importances = best_model.feature_importances_\n",
                "    indices = np.argsort(importances)[::-1]\n",
                "    \n",
                "    # Top 20 features\n",
                "    top_n = 20\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    plt.title(f'Top {top_n} Feature Importances ({best_model_name})')\n",
                "    plt.bar(range(top_n), importances[indices[:top_n]], align='center')\n",
                "    plt.xticks(range(top_n), [X_train.columns[i] for i in indices[:top_n]], rotation=90)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(f\"{best_model_name} does not provide feature importances directly.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_path = config.MODEL_DIR / f\"best_model_{best_model_name.replace(' ', '_').lower()}.pkl\"\n",
                "joblib.dump(best_model, save_path)\n",
                "print(f\"‚úì Saved best model to: {save_path}\")\n",
                "\n",
                "# Also save as generic 'best_model.pkl' for app usage\n",
                "joblib.dump(best_model, config.MODEL_DIR / \"best_model.pkl\")\n",
                "print(f\"‚úì Saved as default model for app.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}