{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Multi-Dataset Merger (v2) - Student Dropout Prediction\n",
    "## Combining 5 Datasets for Maximum Accuracy\n",
    "\n",
    "**Updates in v2:**\n",
    "- **Smart Feature Mapping:** Maps different column names (e.g., 'Medu' -> 'Mother_Qualification') to a standard set.\n",
    "- **Deduplication:** Removes duplicate records if datasets are identical.\n",
    "- **Robust Loading:** Handles different CSV separators.\n",
    "- **Target Standardization:** Unifies target values to 'Dropout', 'Graduate', 'Enrolled'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:59:24.969123Z",
     "iopub.status.busy": "2025-11-23T00:59:24.968979Z",
     "iopub.status.idle": "2025-11-23T00:59:25.349234Z",
     "shell.execute_reply": "2025-11-23T00:59:25.348335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "import config\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:59:25.371205Z",
     "iopub.status.busy": "2025-11-23T00:59:25.370925Z",
     "iopub.status.idle": "2025-11-23T00:59:25.498818Z",
     "shell.execute_reply": "2025-11-23T00:59:25.497745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Higher Education Predictors              - 4,424 rows √ó 35 columns\n",
      "‚úì Student Performance                      -   395 rows √ó 33 columns\n",
      "‚úì Academic Success                         - 4,424 rows √ó 37 columns\n",
      "‚úì Mental Health                            -   101 rows √ó 11 columns\n",
      "‚úì Predict Dropout & Success                - 4,424 rows √ó 37 columns\n",
      "\n",
      "üìä Total datasets loaded: 5\n"
     ]
    }
   ],
   "source": [
    "# Load each dataset with smart separator detection\n",
    "datasets = {}\n",
    "\n",
    "dataset_info = [\n",
    "    ('dataset1', config.DATASET_1_PATH),\n",
    "    ('dataset2', config.DATASET_2_PATH),\n",
    "    ('dataset3', config.DATASET_3_PATH),\n",
    "    ('dataset4', config.DATASET_4_PATH),\n",
    "    ('dataset5', config.DATASET_5_PATH)\n",
    "]\n",
    "\n",
    "for name, path in dataset_info:\n",
    "    try:\n",
    "        # Try loading with default comma separator\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # If only 1 column found, try semicolon separator\n",
    "        if df.shape[1] == 1:\n",
    "            df = pd.read_csv(path, sep=';')\n",
    "            \n",
    "        datasets[name] = df\n",
    "        print(f\"‚úì {config.DATASET_NAMES[name]:40} - {df.shape[0]:5,} rows √ó {df.shape[1]:2} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó {config.DATASET_NAMES[name]:40} - FILE NOT FOUND\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {config.DATASET_NAMES[name]:40} - ERROR: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüìä Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standardize Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:59:25.501359Z",
     "iopub.status.busy": "2025-11-23T00:59:25.501153Z",
     "iopub.status.idle": "2025-11-23T00:59:25.513623Z",
     "shell.execute_reply": "2025-11-23T00:59:25.512846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Higher Education Predictors...\n",
      "  Identified target column: 'Target'\n",
      "‚úì Target standardized. Values: ['Dropout' 'Graduate' 'Enrolled']\n",
      "\n",
      "Processing Student Performance...\n",
      "  ‚ÑπÔ∏è Detected 'G3' (Grade) column. Creating Target from grades.\n",
      "‚úì Target standardized. Values: ['Dropout' 'Graduate']\n",
      "\n",
      "Processing Academic Success...\n",
      "  Identified target column: 'Target'\n",
      "‚úì Target standardized. Values: ['Dropout' 'Graduate' 'Enrolled']\n",
      "\n",
      "Processing Mental Health...\n",
      "  ‚ö†Ô∏è Dataset 'Mental Health' appears to be the Mental Health dataset with no Dropout target.\n",
      "  ‚ö†Ô∏è It will be excluded from training but used for analysis.\n",
      "‚úó No target found\n",
      "\n",
      "Processing Predict Dropout & Success...\n",
      "  Identified target column: 'Target'\n",
      "‚úì Target standardized. Values: ['Dropout' 'Graduate' 'Enrolled']\n"
     ]
    }
   ],
   "source": [
    "def standardize_target(df, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Standardize target variable to: Dropout, Graduate, Enrolled\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    target_col = None\n",
    "    \n",
    "    # --- SPECIAL CASES ---\n",
    "    \n",
    "    # Case 1: Student Performance Dataset (Uses G3 grades)\n",
    "    if 'G3' in df_copy.columns:\n",
    "        print(f\"  ‚ÑπÔ∏è Detected 'G3' (Grade) column. Creating Target from grades.\")\n",
    "        # Rule: G3 < 10 is Fail (Dropout), G3 >= 10 is Pass (Graduate)\n",
    "        df_copy['Target'] = df_copy['G3'].apply(lambda x: 'Dropout' if x < 10 else 'Graduate')\n",
    "        return df_copy\n",
    "\n",
    "    # Case 2: Mental Health Dataset (No direct target)\n",
    "    if 'Do you have Depression?' in df_copy.columns and 'Target' not in df_copy.columns:\n",
    "        print(f\"  ‚ö†Ô∏è Dataset '{dataset_name}' appears to be the Mental Health dataset with no Dropout target.\")\n",
    "        print(f\"  ‚ö†Ô∏è It will be excluded from training but used for analysis.\")\n",
    "        return df_copy\n",
    "\n",
    "    # --- GENERAL DETECTION ---\n",
    "    if target_col is None:\n",
    "        if 'Target' in df_copy.columns:\n",
    "            target_col = 'Target'\n",
    "        elif 'target' in df_copy.columns:\n",
    "            target_col = 'target'\n",
    "        else:\n",
    "            target_candidates = ['Dropout', 'Status', 'dropout', 'status']\n",
    "            for candidate in target_candidates:\n",
    "                matches = [col for col in df_copy.columns if candidate in col]\n",
    "                matches = [m for m in matches if 'marital' not in m.lower()]\n",
    "                matches = [m for m in matches if 'Pstatus' not in m]\n",
    "                if matches:\n",
    "                    target_col = matches[0]\n",
    "                    break\n",
    "    \n",
    "    if target_col is None:\n",
    "        print(f\"‚ö†Ô∏è No target column found. Skipping...\")\n",
    "        return df_copy\n",
    "    \n",
    "    print(f\"  Identified target column: '{target_col}'\")\n",
    "    \n",
    "    if target_col != 'Target':\n",
    "        if 'Target' in df_copy.columns:\n",
    "            df_copy = df_copy.drop(columns=['Target'])\n",
    "        df_copy = df_copy.rename(columns={target_col: 'Target'})\n",
    "    \n",
    "    # Standardize values\n",
    "    df_copy['Target'] = df_copy['Target'].astype(str).str.strip()\n",
    "    return df_copy\n",
    "\n",
    "standardized_datasets = {}\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {config.DATASET_NAMES[name]}...\")\n",
    "    standardized_datasets[name] = standardize_target(df, config.DATASET_NAMES[name])\n",
    "    if 'Target' in standardized_datasets[name].columns:\n",
    "        print(f\"‚úì Target standardized. Values: {standardized_datasets[name]['Target'].unique()[:5]}\")\n",
    "    else:\n",
    "        print(f\"‚úó No target found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Harmonization (Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:59:25.515638Z",
     "iopub.status.busy": "2025-11-23T00:59:25.515464Z",
     "iopub.status.idle": "2025-11-23T00:59:25.534055Z",
     "shell.execute_reply": "2025-11-23T00:59:25.533079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonizing and Mapping Features...\n",
      "\n",
      "‚úì Higher Education Predictors              - Mapped 17 features\n",
      "‚úì Student Performance                      - Mapped 17 features\n",
      "‚úì Academic Success                         - Mapped 17 features\n",
      "- Mental Health                            - Skipped (No Target)\n",
      "‚úì Predict Dropout & Success                - Mapped 17 features\n"
     ]
    }
   ],
   "source": [
    "# Define Standard Feature Names\n",
    "STANDARD_FEATURES = [\n",
    "    'Age',\n",
    "    'Gender',\n",
    "    'Marital_Status',\n",
    "    'Course',\n",
    "    'Mother_Qualification',\n",
    "    'Father_Qualification',\n",
    "    'Previous_Qualification',\n",
    "    'Admission_Grade',\n",
    "    'Displaced',\n",
    "    'Debtor',\n",
    "    'Tuition_Fees_Up_To_Date',\n",
    "    'Scholarship_Holder',\n",
    "    'Unemployment_Rate',\n",
    "    'Inflation_Rate',\n",
    "    'GDP',\n",
    "    'Target'\n",
    "]\n",
    "\n",
    "# Define Mappings for specific dataset types\n",
    "def get_mapping(df_columns):\n",
    "    mapping = {}\n",
    "    cols = [c.lower() for c in df_columns]\n",
    "    \n",
    "    # 1. Academic Dataset (Datasets 1, 3, 5)\n",
    "    if 'mother\\'s qualification' in cols or 'mother qualification' in cols:\n",
    "        mapping = {\n",
    "            'Age at enrollment': 'Age',\n",
    "            'Gender': 'Gender',\n",
    "            'Marital status': 'Marital_Status',\n",
    "            'Course': 'Course',\n",
    "            'Mother\\'s qualification': 'Mother_Qualification',\n",
    "            'Father\\'s qualification': 'Father_Qualification',\n",
    "            'Previous qualification': 'Previous_Qualification',\n",
    "            'Admission grade': 'Admission_Grade',\n",
    "            'Displaced': 'Displaced',\n",
    "            'Debtor': 'Debtor',\n",
    "            'Tuition fees up to date': 'Tuition_Fees_Up_To_Date',\n",
    "            'Scholarship holder': 'Scholarship_Holder',\n",
    "            'Unemployment rate': 'Unemployment_Rate',\n",
    "            'Inflation rate': 'Inflation_Rate',\n",
    "            'GDP': 'GDP',\n",
    "            'Target': 'Target'\n",
    "        }\n",
    "    \n",
    "    # 2. Student Performance Dataset (Dataset 2)\n",
    "    elif 'medu' in cols and 'fedu' in cols:\n",
    "        mapping = {\n",
    "            'age': 'Age',\n",
    "            'sex': 'Gender',\n",
    "            'Medu': 'Mother_Qualification',\n",
    "            'Fedu': 'Father_Qualification',\n",
    "            'address': 'Displaced', # Proxy: U/R might correlate\n",
    "            'paid': 'Tuition_Fees_Up_To_Date', # Proxy\n",
    "            'Target': 'Target'\n",
    "        }\n",
    "        \n",
    "    # 3. Mental Health Dataset (Dataset 4)\n",
    "    elif 'choose your gender' in cols:\n",
    "        mapping = {\n",
    "            'Age': 'Age',\n",
    "            'Choose your gender': 'Gender',\n",
    "            'What is your course?': 'Course',\n",
    "            'Marital status': 'Marital_Status'\n",
    "        }\n",
    "        \n",
    "    return mapping\n",
    "\n",
    "def harmonize_dataset(df, dataset_name):\n",
    "    # Get appropriate mapping\n",
    "    mapping = get_mapping(df.columns)\n",
    "    \n",
    "    # Create new dataframe with standard columns\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Dataset_Source'] = [dataset_name] * len(df)\n",
    "    \n",
    "    # Apply mapping\n",
    "    for original_col, standard_col in mapping.items():\n",
    "        if original_col in df.columns:\n",
    "            new_df[standard_col] = df[original_col]\n",
    "    \n",
    "    # Handle Gender Normalization (Male/Female -> 1/0)\n",
    "    if 'Gender' in new_df.columns:\n",
    "        # If string 'M'/'F' or 'Male'/'Female'\n",
    "        if new_df['Gender'].dtype == 'O':\n",
    "            new_df['Gender'] = new_df['Gender'].apply(lambda x: 1 if str(x).lower().startswith('m') else 0)\n",
    "\n",
    "    # Handle Displaced Normalization (U/R -> 1/0 or Yes/No -> 1/0)\n",
    "    if 'Displaced' in new_df.columns:\n",
    "        if new_df['Displaced'].dtype == 'O':\n",
    "            # Map 'U' (Urban) to 1, 'R' (Rural) to 0? Or 'Yes'/'No'?\n",
    "            # Assuming Displaced means \"Living away from home\".\n",
    "            # If 'address' (U/R) was mapped here, U=Urban, R=Rural.\n",
    "            # Let's assume U=1, R=0 for now, or check values.\n",
    "            # Safest is to map 'yes'/'no' if present.\n",
    "            new_df['Displaced'] = new_df['Displaced'].apply(lambda x: 1 if str(x).lower() in ['yes', 'u', 'urban', '1'] else 0)\n",
    "\n",
    "    # Handle Tuition Fees Normalization (Yes/No -> 1/0)\n",
    "    if 'Tuition_Fees_Up_To_Date' in new_df.columns:\n",
    "        if new_df['Tuition_Fees_Up_To_Date'].dtype == 'O':\n",
    "            new_df['Tuition_Fees_Up_To_Date'] = new_df['Tuition_Fees_Up_To_Date'].apply(lambda x: 1 if str(x).lower() in ['yes', 'paid', '1'] else 0)\n",
    "            \n",
    "    # Handle Scholarship Holder Normalization (Yes/No -> 1/0)\n",
    "    if 'Scholarship_Holder' in new_df.columns:\n",
    "        if new_df['Scholarship_Holder'].dtype == 'O':\n",
    "            new_df['Scholarship_Holder'] = new_df['Scholarship_Holder'].apply(lambda x: 1 if str(x).lower() in ['yes', '1'] else 0)\n",
    "            \n",
    "    # Handle Debtor Normalization (Yes/No -> 1/0)\n",
    "    if 'Debtor' in new_df.columns:\n",
    "        if new_df['Debtor'].dtype == 'O':\n",
    "            new_df['Debtor'] = new_df['Debtor'].apply(lambda x: 1 if str(x).lower() in ['yes', '1'] else 0)\n",
    "            \n",
    "    # Fill missing standard columns with NaN\n",
    "    for col in STANDARD_FEATURES:\n",
    "        if col not in new_df.columns:\n",
    "            new_df[col] = np.nan\n",
    "            \n",
    "    return new_df\n",
    "\n",
    "print(\"Harmonizing and Mapping Features...\\n\")\n",
    "harmonized_list = []\n",
    "for name, df in standardized_datasets.items():\n",
    "    if 'Target' in df.columns:  # Only include datasets with targets\n",
    "        harmonized_df = harmonize_dataset(df, config.DATASET_NAMES[name])\n",
    "        harmonized_list.append(harmonized_df)\n",
    "        print(f\"‚úì {config.DATASET_NAMES[name]:40} - Mapped {harmonized_df.shape[1]} features\")\n",
    "    else:\n",
    "        print(f\"- {config.DATASET_NAMES[name]:40} - Skipped (No Target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge and Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:59:25.536067Z",
     "iopub.status.busy": "2025-11-23T00:59:25.535887Z",
     "iopub.status.idle": "2025-11-23T00:59:25.584385Z",
     "shell.execute_reply": "2025-11-23T00:59:25.583464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Record Count: 13,667\n",
      "Duplicates Removed:   4,654\n",
      "Final Record Count:   9,013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Saved to D:\\Santosh_minor\\notebooks\\..\\data\\processed\\merged_datasets.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatenate\n",
    "merged_df = pd.concat(harmonized_list, ignore_index=True)\n",
    "initial_count = len(merged_df)\n",
    "\n",
    "# Drop Duplicates (Ignoring Source column)\n",
    "# This handles the case where Dataset 1, 3, and 5 are identical\n",
    "cols_to_check = [c for c in merged_df.columns if c != 'Dataset_Source']\n",
    "merged_df = merged_df.drop_duplicates(subset=cols_to_check)\n",
    "final_count = len(merged_df)\n",
    "\n",
    "print(f\"\\nInitial Record Count: {initial_count:,}\")\n",
    "print(f\"Duplicates Removed:   {initial_count - final_count:,}\")\n",
    "print(f\"Final Record Count:   {final_count:,}\")\n",
    "\n",
    "# Save\n",
    "output_path = config.MERGED_DATASET_PATH\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n‚úì Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T00:59:25.586623Z",
     "iopub.status.busy": "2025-11-23T00:59:25.586196Z",
     "iopub.status.idle": "2025-11-23T00:59:25.596352Z",
     "shell.execute_reply": "2025-11-23T00:59:25.595712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values (%):\n",
      "Admission_Grade            50.926440\n",
      "Course                      3.139909\n",
      "Previous_Qualification      3.139909\n",
      "Marital_Status              3.139909\n",
      "Unemployment_Rate           3.139909\n",
      "Inflation_Rate              3.139909\n",
      "Scholarship_Holder          3.139909\n",
      "Debtor                      3.139909\n",
      "GDP                         3.139909\n",
      "Father_Qualification        0.000000\n",
      "Gender                      0.000000\n",
      "Age                         0.000000\n",
      "Mother_Qualification        0.000000\n",
      "Displaced                   0.000000\n",
      "Tuition_Fees_Up_To_Date     0.000000\n",
      "Target                      0.000000\n",
      "dtype: float64\n",
      "\n",
      "Target Distribution:\n",
      "Target\n",
      "Graduate    4493\n",
      "Dropout     2939\n",
      "Enrolled    1581\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "print(\"\\nMissing Values (%):\")\n",
    "print((merged_df[STANDARD_FEATURES].isnull().sum() / len(merged_df) * 100).sort_values(ascending=False))\n",
    "\n",
    "if 'Target' in merged_df.columns:\n",
    "    print(\"\\nTarget Distribution:\")\n",
    "    print(merged_df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
