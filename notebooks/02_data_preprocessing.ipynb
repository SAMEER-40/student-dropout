{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Data Preprocessing & Feature Engineering\n",
    "## Student Dropout Prediction Project\n",
    "\n",
    "**Goal:** Prepare the merged dataset for machine learning by handling missing values, encoding features, scaling, and balancing classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T01:02:40.187918Z",
     "iopub.status.busy": "2025-11-23T01:02:40.187761Z",
     "iopub.status.idle": "2025-11-23T01:02:41.645841Z",
     "shell.execute_reply": "2025-11-23T01:02:41.645230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "import config\n",
    "importlib.reload(config)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T01:02:41.667346Z",
     "iopub.status.busy": "2025-11-23T01:02:41.666972Z",
     "iopub.status.idle": "2025-11-23T01:02:41.682062Z",
     "shell.execute_reply": "2025-11-23T01:02:41.681477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset loaded: 9,013 rows, 17 columns\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(config.MERGED_DATASET_PATH)\n",
    "    print(f\"âœ“ Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Error: Merged dataset not found. Run 00_merge_datasets_v2.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T01:02:41.683874Z",
     "iopub.status.busy": "2025-11-23T01:02:41.683663Z",
     "iopub.status.idle": "2025-11-23T01:02:41.700358Z",
     "shell.execute_reply": "2025-11-23T01:02:41.699713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Features: 15\n",
      "Categorical Features: 0\n",
      "âœ“ Missing values imputed\n",
      "Remaining missing: 0\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['Target', 'Dataset_Source'])\n",
    "y = df['Target']\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"Numerical Features: {len(numerical_cols)}\")\n",
    "print(f\"Categorical Features: {len(categorical_cols)}\")\n",
    "\n",
    "# Define Imputers\n",
    "# Numerical -> Median (robust to outliers)\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Categorical -> Most Frequent (Mode)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply Imputation (only if columns exist)\n",
    "if len(numerical_cols) > 0:\n",
    "    X[numerical_cols] = num_imputer.fit_transform(X[numerical_cols])\n",
    "    \n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "print(\"âœ“ Missing values imputed\")\n",
    "print(f\"Remaining missing: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Encoding & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T01:02:41.701956Z",
     "iopub.status.busy": "2025-11-23T01:02:41.701798Z",
     "iopub.status.idle": "2025-11-23T01:02:41.712214Z",
     "shell.execute_reply": "2025-11-23T01:02:41.711514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Classes: ['Dropout' 'Enrolled' 'Graduate']\n",
      "âœ“ Data processed. New shape: (9013, 15)\n"
     ]
    }
   ],
   "source": [
    "# Encode Target (Multi-class)\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Target Classes: {target_encoder.classes_}\")\n",
    "\n",
    "# Create Preprocessor (ColumnTransformer)\n",
    "transformers = []\n",
    "\n",
    "# Add numerical transformer\n",
    "if len(numerical_cols) > 0:\n",
    "    transformers.append(('num', Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numerical_cols))\n",
    "\n",
    "# Add categorical transformer (only if categorical columns exist)\n",
    "if len(categorical_cols) > 0:\n",
    "    transformers.append(('cat', Pipeline([\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]), categorical_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = []\n",
    "if len(numerical_cols) > 0:\n",
    "    feature_names.extend(numerical_cols.tolist())\n",
    "if len(categorical_cols) > 0:\n",
    "    feature_names.extend(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "print(f\"âœ“ Data processed. New shape: {X_processed_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T01:02:41.713873Z",
     "iopub.status.busy": "2025-11-23T01:02:41.713698Z",
     "iopub.status.idle": "2025-11-23T01:02:41.721040Z",
     "shell.execute_reply": "2025-11-23T01:02:41.720200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Split complete\n",
      "Train: (7210, 15), Test: (1803, 15)\n",
      "Class Distribution (Train): [2351 1265 3594]\n",
      "Class Distribution (Test):  [588 316 899]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed_df,\n",
    "    y_encoded,\n",
    "    test_size=config.TEST_SIZE,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Split complete\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Class Distribution (Train): {np.bincount(y_train)}\")\n",
    "print(f\"Class Distribution (Test):  {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Class Imbalance (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T01:02:41.722648Z",
     "iopub.status.busy": "2025-11-23T01:02:41.722451Z",
     "iopub.status.idle": "2025-11-23T01:02:41.818845Z",
     "shell.execute_reply": "2025-11-23T01:02:41.818331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SMOTE applied\n",
      "Original Train: (7210, 15)\n",
      "Balanced Train: (10782, 15)\n",
      "Balanced Class Distribution: [3594 3594 3594]\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=config.RANDOM_STATE)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"âœ“ SMOTE applied\")\n",
    "print(f\"Original Train: {X_train.shape}\")\n",
    "print(f\"Balanced Train: {X_train_balanced.shape}\")\n",
    "print(f\"Balanced Class Distribution: {np.bincount(y_train_balanced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T01:02:41.820583Z",
     "iopub.status.busy": "2025-11-23T01:02:41.820421Z",
     "iopub.status.idle": "2025-11-23T01:02:41.953363Z",
     "shell.execute_reply": "2025-11-23T01:02:41.952811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Processed data saved:\n",
      "  Train: D:\\Santosh_minor\\notebooks\\..\\data\\processed\\train_data.csv\n",
      "  Test:  D:\\Santosh_minor\\notebooks\\..\\data\\processed\\test_data.csv\n",
      "âœ“ Preprocessor saved: D:\\Santosh_minor\\notebooks\\..\\models\\preprocessor.pkl\n",
      "âœ“ Target Encoder saved: D:\\Santosh_minor\\notebooks\\..\\models\\target_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Convert back to DataFrame\n",
    "train_df = pd.DataFrame(X_train_balanced, columns=feature_names)\n",
    "train_df['Target'] = y_train_balanced\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "test_df['Target'] = y_test\n",
    "\n",
    "# Save\n",
    "train_df.to_csv(config.TRAIN_DATA_PATH, index=False)\n",
    "test_df.to_csv(config.TEST_DATA_PATH, index=False)\n",
    "\n",
    "# Save Preprocessor & Encoder\n",
    "joblib.dump(preprocessor, config.MODEL_DIR / \"preprocessor.pkl\")\n",
    "joblib.dump(target_encoder, config.MODEL_DIR / \"target_encoder.pkl\")\n",
    "\n",
    "print(f\"\\nâœ“ Processed data saved:\")\n",
    "print(f\"  Train: {config.TRAIN_DATA_PATH}\")\n",
    "print(f\"  Test:  {config.TEST_DATA_PATH}\")\n",
    "print(f\"âœ“ Preprocessor saved: {config.MODEL_DIR / 'preprocessor.pkl'}\")\n",
    "print(f\"âœ“ Target Encoder saved: {config.MODEL_DIR / 'target_encoder.pkl'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
