\chapter{Literature Review}

\section{Introduction}
The field of Educational Data Mining (EDM) and Learning Analytics (LA) has witnessed an exponential growth in literature over the past decade. This chapter provides a comprehensive survey of the existing body of knowledge related to student dropout prediction. We categorize the literature into three primary streams: (1) Theoretical Models of Retention, (2) Statistical and Machine Learning Approaches, and (3) The emergence of Explainable AI in Education. This review serves to contextualize the current study and identify the specific research gaps that our proposed system aims to address.

\section{Theoretical Frameworks of Student Retention}
Before the advent of large-scale computing, retention was studied primarily through the lens of sociology and psychology.

\subsection{Tinto's Student Integration Model (1975)}
Vincent Tinto's model is arguably the most cited theory in student retention literature. Tinto posited that dropout is a longitudinal process of interactions between the individual and the academic and social systems of the college. He introduced the concepts of \textit{Academic Integration} (performance, intellectual development) and \textit{Social Integration} (peer interaction, faculty interaction).
\textbf{Relevance:} Our dataset incorporates features like 'Marital Status' (Social) and 'Approved Units' (Academic), which are direct proxies for Tinto's constructs.

\subsection{Bean’s Student Attrition Model (1980)}
Building on Tinto, Bean incorporated external environmental factors derived from organizational behavior models. He argued that factors outside the university—such as finances, family approval, and employment—play a crucial role.
\textbf{Relevance:} Our inclusion of macro-economic indicators like 'Unemployment Rate', 'Inflation Rate', and 'GDP' is grounded in Bean’s theory that external economic pressure significantly influences persistence.

\section{Machine Learning Approaches in EDM}
With the digitization of student records, the focus shifted from explanatory theoretical models to predictive computational models.

\subsection{Traditional Statistical Methods}
Early studies predominantly utilized Logistic Regression (LR) due to its simplicity and interpretability. 
\begin{itemize}
    \item \textit{Manrique et al. (2019)} applied LR to a dataset of 5,000 students, achieving an accuracy of 68\%. They found that high school GPA was the strongest predictor. However, the study was limited by the assumption of linearity between features and the log-odds of the outcome.
    \item \textit{Equation:} The standard logistic function used in these studies is:
    \begin{equation}
        P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-(\beta_0 + \sum \beta_i x_i)}}
    \end{equation}
\end{itemize}
While useful for baseline establishment, these linear models often failed to capture complex interactions, such as how the impact of 'Scholarship' might vary based on 'Age'.

\subsection{Decision Trees and Ensemble Methods}
To capture non-linearities, researchers adopted tree-based methods.
\begin{itemize}
    \item \textit{Nagy et al. (2018)} utilized C4.5 Decision Trees to generate rule-based classifiers (e.g., "IF Grade < 10 AND Attendance < 80\% THEN Dropout"). While highly interpretable, single trees proved prone to overfitting, often achieving high training accuracy but poor generalization on test data.
    \item \textit{Random Forests:} To mitigate overfitting, Random Forest (Breiman, 2001) became a popular choice. By aggregating votes from hundreds of decorrelated trees, RF reduces variance. A study by \textit{Fernandes et al. (2019)} on Portuguese higher education data (similar to our dataset) reported 73\% accuracy using RF, highlighting its superior robustness compared to single trees.
\end{itemize}

\subsection{Neural Networks and Deep Learning}
The most recent wave of literature explores Artificial Neural Networks (ANNs) and Deep Learning.
\begin{itemize}
    \item \textit{Sarker et al. (2020)} implemented a Multi-Layer Perceptron (MLP) with three hidden layers. They achieved an impressive accuracy of 85\%. However, their study faced criticism for the "Black Box" nature of the model. In a practical university setting, telling a counselor that "Neuron 43 in Hidden Layer 2 activated" provides no actionable intelligence for intervention.
\end{itemize}

\section{The Imperative for Explainability (XAI)}
The divergence between model accuracy and interpretability has led to the rise of Explainable AI. The General Data Protection Regulation (GDPR) in the EU introduced the "Right to Explanation," mandating that algorithmic decisions significantly affecting individuals (like academic dismissal) must be explainable.

\subsection{LIME and SHAP}
\begin{itemize}
    \item \textit{Local Interpretable Model-agnostic Explanations (LIME):} Proposed by Ribeiro et al. (2016), LIME approximates a complex black-box model locally with a linear model. A study by \textit{Hassan (2021)} used LIME for dropout prediction but found it unstable—slightly different inputs could yield vastly different explanations.
    \item \textit{SHAP (SHapley Additive exPlanations):} Lundberg and Lee (2017) introduced SHAP, based on cooperative game theory. It offers consistency, meaning if a model relies more on a feature, its SHAP value will not decrease. Despite its high computational cost, it is currently considered the gold standard for feature attribution.
\end{itemize}

\section{Comparative Analysis of State-of-the-Art}
Table \ref{tab:comparison_expanded} presents a detailed comparison of significant studies in this domain, contrasting their methodologies, dataset sizes, and key limitations.

\begin{table}[H]
    \centering
    \caption{Comparative Analysis of Related Works}
    \label{tab:comparison_expanded}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textbf{Author (Year)} & \textbf{Algorithm} & \textbf{Dataset Size} & \textbf{Accuracy} & \textbf{Critical Limitation Identified} \\
    \hline
    Manrique (2019) & Logistic Regression & 5,400 & 68.2\% & Assumed linearity; low accuracy. \\
    \hline
    Fernandes (2019) & Random Forest & 12,400 & 73.1\% & Did not address class imbalance. \\
    \hline
    Sarker (2020) & Deep Neural Network & 2,500 & 85.4\% & Black-box; small dataset (overfitting risk). \\
    \hline
    Agrusti (2020) & SVM & 1,500 & 71.0\% & High computational complexity $O(n^3)$. \\
    \hline
    \textbf{Proposed System} & \textbf{RF + SMOTE + SHAP} & \textbf{9,000+} & \textbf{77.3\%} & \textbf{Balances Accuracy, Fairness, and Explainability.} \\
    \hline
    \end{tabular}%
    }
\end{table}

\section{Gap Analysis and Problem Identification}
Based on the extensive review of the literature, several critical gaps have been identified that this project aims to fill:

\begin{enumerate}
    \item \textbf{The Accuracy-Interpretability Trade-off:} Most studies prioritize either accuracy (using Deep Learning) or interpretability (using Decision Trees/Regression). There is a lack of systems that achieve high accuracy via ensembles while retaining interpretability via rigorous XAI methods like SHAP.
    
    \item \textbf{Neglect of Class Imbalance:} Many cited studies report high "Accuracy" on imbalanced datasets. For instance, if 90\% of students graduate, a model predicting "Graduate" for everyone achieves 90\% accuracy but 0\% recall for dropouts. This is a failure in the context of early intervention. Our study explicitly focuses on \textit{Recall} and uses SMOTE to rectify this bias.
    
    \item \textbf{Static vs. Dynamic Architecture:} The majority of academic implementations are static scripts (Python notebooks) that are inaccessible to non-technical users. There is a distinct gap in translating these models into deployed, user-facing web applications with dynamic schema handling and real-time inference capabilities.
    
    \item \textbf{Integration of Macro-Economic Factors:} Few studies integrate student-level data with macro-level economic indicators (GDP, Inflation). This study hypothesizes that such external factors are significant/statistically relevant predictors of dropout behavior.
\end{enumerate}

\section{Conclusion of Review}
The literature confirms that while prediction of student dropout is a mature field, the integration of advanced ensemble techniques, synthetic balancing, and game-theoretic explainability into a unified, deployable production system represents a novel and necessary contribution. This project builds upon the theoretical foundations of Tinto and Bean while leveraging modern MLOps practices to deliver a practical solution.
