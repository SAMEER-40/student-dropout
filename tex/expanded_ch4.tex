\chapter{Proposed Methodology}

\section{Introduction}
The methodology adopted for this project follows the standard Cross-Industry Standard Process for Data Mining (CRISP-DM) lifecycle. This involves iterative phases of Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment. This chapter details the algorithmic approaches utilized in each of these phases, providing formal pseudocode for reproducibility.

\section{Data Acquisition and Integration}
The primary dataset is derived from the UCI Machine Learning Repository, titled "Predict Students' Dropout and Academic Success". It aggregates information from disjoint databases (Portalegre Polytechnic University) covering three domains:
\begin{itemize}
    \item \textbf{Academic Path:} Grades, enrolled units, evaluations.
    \item \textbf{Socio-Demographic:} Age, gender, marital status.
    \item \textbf{Macro-Economic:} GDP, inflation, unemployment rate.
\end{itemize}
The raw data consists of 4,424 instances with 36 distinct attributes.

\section{Data Preprocessing Framework}
Real-world educational data is often noisy, incomplete, and heterogeneous. A robust preprocessing pipeline is essential to convert this raw data into a format suitable for machine learning algorithms.

\subsection{Algorithm 1: Adaptive Data Preprocessing}
We implemented a dynamic preprocessing pipeline using the \texttt{ColumnTransformer} pattern. This ensures that the exact same transformations applied during training are applied during inference, preventing "Training-Serving Skew".

\begin{algorithm}[H]
\caption{Adaptive Data Preprocessing Strategy}
\label{alg:preprocessing_expanded}
\begin{algorithmic}[1]
\Require Raw Dataset $\mathcal{D}_{raw}$, Categorical Features $C$, Numerical Features $N$
\Ensure Processed Matrix $X_{processed}$, Transformation Pipeline $\Psi$

\State \textbf{Initialize} Separate pipelines for numeric and categorical data
\State $\Psi_{num} \leftarrow$ \text{Pipeline}([
    \State \quad ('imputer', SimpleImputer(strategy='mean')),
    \State \quad ('scaler', StandardScaler())
\State ])

\State $\Psi_{cat} \leftarrow$ \text{Pipeline}([
    \State \quad ('imputer', SimpleImputer(strategy='constant', fill='Unknown')),
    \State \quad ('encoder', OneHotEncoder(handle\_unknown='ignore'))
\State ])

\State \textbf{Initialize} ColumnTransformer $\Psi$
\State $\Psi \leftarrow$ \text{Compose}([
    \State \quad ('num', $\Psi_{num}, N$),
    \State \quad ('cat', $\Psi_{cat}, C$)
\State ])

\State \textbf{Fit} $\Psi$ on $\mathcal{D}_{raw}$ to learn statistics ($\mu, \sigma$) and vocabulary
\State $X_{processed} \leftarrow \Psi.transform(\mathcal{D}_{raw})$
\State \Return $X_{processed}, \Psi$
\end{algorithmic}
\end{algorithm}

\section{Handling Class Imbalance: SMOTE}
An analysis of the target variable $y$ revealed a significant class imbalance:
\begin{itemize}
    \item Graduate: 49.9\%
    \item Dropout: 32.1\%
    \item Enrolled: 18.0\%
\end{itemize}
Standard classifiers optimizing for accuracy would bias towards the 'Graduate' class. To address this, we employed the Synthetic Minority Over-sampling Technique (SMOTE).

\subsection{Mathematical Basis of SMOTE}
Unlike naive oversampling which duplicates records (leading to overfitting), SMOTE synthesizes new instances in the feature space. For a minority sample $\mathbf{x}_i$, a new sample $\mathbf{x}_{new}$ is generated by interpolating between $\mathbf{x}_i$ and one of its $k$-nearest neighbors $\mathbf{x}_{zi}$:
\begin{equation}
    \mathbf{x}_{new} = \mathbf{x}_i + \lambda \times (\mathbf{x}_{zi} - \mathbf{x}_i)
\end{equation}
where $\lambda$ is a random variable $\in [0, 1]$.

\begin{algorithm}[H]
\caption{SMOTE Synthetic Generation}
\label{alg:smote_expanded}
\begin{algorithmic}[1]
\Require Minority Class Set $S_{min}$, Percentage $P$, Neighborhood size $k$
\Ensure Synthetic Set $S_{syn}$

\State $T \leftarrow (P/100) \times |S_{min}|$ \Comment{Number of samples to generate}
\State $S_{syn} \leftarrow \emptyset$

\While{$|S_{syn}| < T$}
    \State Select random sample $\mathbf{x}_i \in S_{min}$
    \State Find $k$-nearest neighbors of $\mathbf{x}_i$ in feature space using Euclidean distance
    \State Select random neighbor $\mathbf{x}_{neighbor}$
    \State Generate random $\lambda \sim U(0, 1)$
    \State $\mathbf{x}_{synthetic} \leftarrow \mathbf{x}_i + \lambda \cdot (\mathbf{x}_{neighbor} - \mathbf{x}_i)$
    \State $S_{syn} \leftarrow S_{syn} \cup \{\mathbf{x}_{synthetic}\}$
\EndWhile

\State \Return $S_{syn}$
\end{algorithmic}
\end{algorithm}

\section{Model Development and Optimization}
We selected Ensemble Learning methods due to their superior performance on tabular data. Specifically, we compared Random Forest (Bagging) and XGBoost (Boosting).

\subsection{Random Forest Classifier}
Random Forest constructs $T$ decision trees $h_1(\mathbf{x}), ..., h_T(\mathbf{x})$. Each tree is trained on a bootstrap sample of the data, and at each split, only a random subset of features is considered. The final prediction is the majority vote:
\begin{equation}
    H(\mathbf{x}) = \text{argmax}_{j} \sum_{t=1}^{T} \mathbb{I}(h_t(\mathbf{x}) = j)
\end{equation}

\subsection{Hyperparameter Optimization Strategy}
Finding the optimal configuration $\theta^*$ for our model is a non-convex optimization problem. We employed Randomized Search Cross-Validation to efficiently explore the hyperparameter space.

\begin{algorithm}[H]
\caption{Randomized Search Cross-Validation}
\label{alg:random_search}
\begin{algorithmic}[1]
\Require Training Data $D_{train}$, Model $M$, Param Distribution $\Omega$, Iterations $N_{iter}$, Folds $K$
\Ensure Best Model $M^*$

\State $Results \leftarrow \emptyset$
\For{$i = 1$ to $N_{iter}$}
    \State Sample hyperparameters $\theta_i \sim \Omega$
    \State Initialize scores $S_i \leftarrow []$
    \State Partition $D_{train}$ into $K$ folds $\{F_1, ..., F_K\}$
    
    \For{$k = 1$ to $K$}
        \State $Train_k \leftarrow D_{train} \setminus F_k$
        \State $Val_k \leftarrow F_k$
        \State Train $M(\theta_i)$ on $Train_k$
        \State Evaluate score $s_{ik}$ on $Val_k$
        \State $S_i.append(s_{ik})$
    \EndFor
    
    \State $\bar{S}_i \leftarrow \text{mean}(S_i)$
    \State $Results.append((\theta_i, \bar{S}_i))$
\EndFor

\State $\theta^* \leftarrow \text{argmax}_{\theta} (Results)$
\State Retrain $M^*$ using $\theta^*$ on full $D_{train}$
\State \Return $M^*$
\end{algorithmic}
\end{algorithm}

\section{Explainability Engine (SHAP)}
To satisfy the "Right to Explanation", we utilize SHAP (SHapley Additive exPlanations). SHAP assigns an importance value $\phi_i$ to each feature for a specific prediction, satisfying the property of local accuracy:
\begin{equation}
    f(x) - \mathbb{E}[f(x)] = \sum_{i=1}^{M} \phi_i
\end{equation}
This means the prediction is the sum of the bias (average prediction) plus the contributions of each feature. This system calculates these values at inference time to generate local explanations.
